## Variational Autoencoder (VAE) & Conditional VAE (CVAE)

The notebook implements **Variational Autoencoder (VAE)** and **Conditional Variational Autoencoder (CVAE)** models using deep learning.  
It covers the theoretical formulation, model architecture, training process, and qualitative evaluation of generated samples, demonstrating how conditional information improves generative performance.

## Neural Discrete Representation Learning (VQ-VAE)

The notebook implements and analyzes **Neural Discrete Representation Learning** using the **Vector Quantized Variational Autoencoder (VQ-VAE)** framework.  
The goal is to learn compact, discrete latent representations by mapping continuous encoder outputs to a finite codebook via vector quantization.

The implementation covers:
- Encoderâ€“decoder architecture
- Vector quantization with codebook learning
- Commitment and reconstruction losses
- Training and evaluation of discrete latent embeddings

This work is useful for representation learning tasks where discrete latent spaces are preferred, such as compression, generative modeling, and symbolic abstraction.
